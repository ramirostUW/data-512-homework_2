{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.1",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "extensions": {
            "azuredatastudio": {
                "version": 1,
                "views": []
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Step 1 - Getting PageInfo Data\n",
                "\n",
                "This short notebook is designed to take in a list of Wikipedia article titles on U.S. cities and query Wikipedia's PageInfo API for its data on those articles. It takes in the list of articles from the \"page\\_title\" column of a CSV on the same directory as this notebook, named \"us\\_cities\\_by\\_state\\_SEPT.2023.csv\". It will store each individual API call reponse in a subdirectory, \"raw\\_API\\_data/Pageinfo/\", with each JSON file named after the corresponding article title."
            ],
            "metadata": {
                "azdata_cell_guid": "7b2a4eff-3ea7-4009-8fc6-768d7ceb84b0"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#this cell uses pandas to import the csv list of articles \r\n",
                "\r\n",
                "import pandas as pd \r\n",
                "article_csv = pd.read_csv(\"input/us_cities_by_state_SEPT.2023.csv\")\r\n",
                "article_list = article_csv['page_title'].tolist()\r\n",
                "article_csv"
            ],
            "metadata": {
                "azdata_cell_guid": "932f2532-2d3c-4ec2-b3d5-15e42b153326"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "         state           page_title  \\\n0      Alabama   Abbeville, Alabama   \n1      Alabama  Adamsville, Alabama   \n2      Alabama     Addison, Alabama   \n3      Alabama       Akron, Alabama   \n4      Alabama   Alabaster, Alabama   \n...        ...                  ...   \n22152  Wyoming   Wamsutter, Wyoming   \n22153  Wyoming   Wheatland, Wyoming   \n22154  Wyoming     Worland, Wyoming   \n22155  Wyoming      Wright, Wyoming   \n22156  Wyoming       Yoder, Wyoming   \n\n                                                     url  \n0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n...                                                  ...  \n22152   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n22153   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n22154     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n22155      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n22156       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n\n[22157 rows x 3 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>page_title</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alabama</td>\n      <td>Abbeville, Alabama</td>\n      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alabama</td>\n      <td>Adamsville, Alabama</td>\n      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alabama</td>\n      <td>Addison, Alabama</td>\n      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Alabama</td>\n      <td>Akron, Alabama</td>\n      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Alabama</td>\n      <td>Alabaster, Alabama</td>\n      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22152</th>\n      <td>Wyoming</td>\n      <td>Wamsutter, Wyoming</td>\n      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n    </tr>\n    <tr>\n      <th>22153</th>\n      <td>Wyoming</td>\n      <td>Wheatland, Wyoming</td>\n      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n    </tr>\n    <tr>\n      <th>22154</th>\n      <td>Wyoming</td>\n      <td>Worland, Wyoming</td>\n      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n    </tr>\n    <tr>\n      <th>22155</th>\n      <td>Wyoming</td>\n      <td>Wright, Wyoming</td>\n      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n    </tr>\n    <tr>\n      <th>22156</th>\n      <td>Wyoming</td>\n      <td>Yoder, Wyoming</td>\n      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n    </tr>\n  </tbody>\n</table>\n<p>22157 rows × 3 columns</p>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 2,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "The below code was was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program, and made available under the [Creative a Commons](https://creativecommons.org/) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - August 14, 2023, and defines a function and accompanying constants needed to query the PageInfo API for an individual article's data. The User Agent constant was modified to include my email address."
            ],
            "metadata": {
                "azdata_cell_guid": "5733bb24-1e26-4e00-abb6-3a2812b33e24"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import time, json, requests\r\n",
                "\r\n",
                "#########\r\n",
                "#\r\n",
                "#    CONSTANTS\r\n",
                "#\r\n",
                "\r\n",
                "# The basic English Wikipedia API endpoint\r\n",
                "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\r\n",
                "\r\n",
                "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\r\n",
                "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\r\n",
                "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\r\n",
                "\r\n",
                "# When making automated requests we should include something that is unique to the person making the request\r\n",
                "# This should include an email - your UW email would be good to put in there\r\n",
                "REQUEST_HEADERS = {\r\n",
                "    'User-Agent': '<ramirost@uw.edu>, University of Washington, Ramiro Steinmann Petrasso',\r\n",
                "}\r\n",
                "\r\n",
                "# This is just a list of English Wikipedia article titles that we can use for example requests\r\n",
                "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\r\n",
                "\r\n",
                "# This is a string of additional page properties that can be returned see the Info documentation for\r\n",
                "# what can be included. If you don't want any this can simply be the empty string\r\n",
                "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\r\n",
                "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\r\n",
                "\r\n",
                "# This template lists the basic parameters for making this\r\n",
                "PAGEINFO_PARAMS_TEMPLATE = {\r\n",
                "    \"action\": \"query\",\r\n",
                "    \"format\": \"json\",\r\n",
                "    \"titles\": \"\",           # to simplify this should be a single page title at a time\r\n",
                "    \"prop\": \"info\",\r\n",
                "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\r\n",
                "}\r\n",
                "\r\n",
                "#########\r\n",
                "#\r\n",
                "#    PROCEDURES/FUNCTIONS\r\n",
                "#\r\n",
                "\r\n",
                "def request_pageinfo_per_article(article_title = None, \r\n",
                "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \r\n",
                "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\r\n",
                "                                 headers = REQUEST_HEADERS):\r\n",
                "    \r\n",
                "    # article title can be as a parameter to the call or in the request_template\r\n",
                "    if article_title:\r\n",
                "        request_template['titles'] = article_title\r\n",
                "\r\n",
                "    if not request_template['titles']:\r\n",
                "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\r\n",
                "\r\n",
                "    # make the request\r\n",
                "    try:\r\n",
                "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\r\n",
                "        # occurs during the request processing - throttling is always a good practice with a free\r\n",
                "        # data source like Wikipedia - or any other community sources\r\n",
                "        if API_THROTTLE_WAIT > 0.0:\r\n",
                "            time.sleep(API_THROTTLE_WAIT)\r\n",
                "        response = requests.get(endpoint_url, headers=headers, params=request_template)\r\n",
                "        json_response = response.json()\r\n",
                "    except Exception as e:\r\n",
                "        print(e)\r\n",
                "        json_response = None\r\n",
                "    return json_response\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "daba100b-dcc9-442d-bb0d-569240f2e8c9",
                "tags": []
            },
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": [
                "#This constant defines the directory in which all of the JSON files will be dumped. \r\n",
                "DATA_STORAGE_DIRECTORY = \"raw_API_data/Pageinfo/\"\r\n",
                "\r\n",
                "\r\n",
                "#This function takes in a Python dict and stores it as a JSON in the specified subdirectory, with the specified title/filename.S\r\n",
                "def dump_to_file(view_data, title, folder):\r\n",
                "    #forbidden characters\r\n",
                "    title = title.replace(\"\\\\\", \"_\")\r\n",
                "    title = title.replace(\":\", \"_\")\r\n",
                "    title = title.replace(\"/\", \"_\")\r\n",
                "    title = title.replace(\"*\", \"_\")\r\n",
                "    title = title.replace('\"', \"_\")\r\n",
                "    title = title.replace(\"<\", \"_\")\r\n",
                "    title = title.replace(\">\", \"_\")\r\n",
                "    title = title.replace(\"?\", \"_\")\r\n",
                "\r\n",
                "\r\n",
                "    #print(\"writing pageview data file for: \", title)\r\n",
                "\r\n",
                "    with open((folder + \"/\" + title + \".json\"), \"w\") as outfile:\r\n",
                "        json.dump(view_data, outfile)\r\n",
                "\r\n",
                "#This function loops over a list of Wikipedia article titles, gets the Pageinfo data for each article, \r\n",
                "#and then writes that data to the specified subdirectory as json files titled after each article. By\r\n",
                "#default it will store the files in the folder from the above constant\r\n",
                "def grab_articles_in_list(list_of_articles,\r\n",
                "                          directory = DATA_STORAGE_DIRECTORY):\r\n",
                "   for article in list_of_articles: \r\n",
                "        #print(f\"Getting page info data for: {article}\")\r\n",
                "        info = request_pageinfo_per_article(article)\r\n",
                "        #print(json.dumps(info,indent=4))\r\n",
                "        dump_to_file(info, article, directory)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "8f956e9d-53ac-44cf-93ed-73ee85b51f24"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#This simple line executes the loop to download and store all of the PageInfo data.\r\n",
                "\r\n",
                "grab_articles_in_list(article_list)"
            ],
            "metadata": {
                "azdata_cell_guid": "c79e9081-bdc8-4eed-9d9e-dfc27e8943ed"
            },
            "outputs": [],
            "execution_count": 1
        }
    ]
}